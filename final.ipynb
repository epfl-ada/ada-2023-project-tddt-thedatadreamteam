{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeoWiki: Investigating Geographical Biases in Wikispeedia\n",
    "\n",
    "*TheDataDreamTeam*\n",
    "\n",
    "This project focuses on investigating geographical biases in the Wikispeedia game and player behavior, using the 2007 Wikipedia Selection for schools dataset as the data source. Our goal is to explore if biases exist towards North America and Europe in article selection and gameplay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.subplots\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "\n",
    "REMOVE_INTERNATIONAL = True\n",
    "INTERNATIONAL_LABEL = \"International\"\n",
    "EUROPE_LABEL = \"Europe\"\n",
    "\n",
    "\n",
    "PLOTS_PATH = \"plots\"\n",
    "PLOTS_PATH_PLT = os.path.join(PLOTS_PATH, \"plt\")\n",
    "PLOTS_PATH_PX = os.path.join(PLOTS_PATH, \"px\")\n",
    "PLOTS_PATH_HTML = os.path.join(PLOTS_PATH, \"html\")\n",
    "\n",
    "FIGURE_WIDTH = 800\n",
    "FIGURE_HEIGHT = 600\n",
    "\n",
    "for path in [PLOTS_PATH_PLT, PLOTS_PATH_PX, PLOTS_PATH_HTML]: \n",
    "    os.makedirs(path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All articles\n",
    "\n",
    "To begin our analysis, we load the dataset containing information about all articles from the Wikispeedia game. The dataset is stored in the file `articles.tsv`.\n",
    "\n",
    "This dataset provides valuable information about all articles in the Wikispeedia game, setting the foundation for further exploration and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_all = pd.read_csv(\n",
    "    os.path.join(\"Data\", \"wikispeedia_paths-and-graph\", \"articles.tsv\"),\n",
    "    delimiter=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"name\"],\n",
    "    skip_blank_lines=True,\n",
    "    comment=\"#\"\n",
    ")\n",
    "\n",
    "display(df_articles_all.head())\n",
    "print(\"Size:\", df_articles_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article continent labels\n",
    "\n",
    "Next, we label the articles with their respective continents using information stored in the `continents.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continents = pd.read_csv(os.path.join(\"Data\", \"continents.csv\"))\n",
    "\n",
    "if REMOVE_INTERNATIONAL:\n",
    "    labeled_articles_all_count = len(df_continents)\n",
    "    df_continents = df_continents[df_continents.continent != INTERNATIONAL_LABEL]\n",
    "    labeled_articles_count = len(df_continents)\n",
    "    print(f\"Removing articles labeled as {INTERNATIONAL_LABEL}, Removed articles: {labeled_articles_all_count - labeled_articles_count}\")\n",
    "\n",
    "display(df_continents.head())\n",
    "print(\"Size:\", df_continents.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset, now labeled with continents, is essential for our geographical analysis. The removal of articles labeled as \"International\" is done as a data preprocessing step. This decision is made to focus the analysis on articles that are distinctly associated with specific continents, making it easier to investigate geographical biases in the Wikispeedia game.\n",
    "\n",
    "The continent labels enable us to explore geographical biases in the Wikispeedia game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by loading information about article categories from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_categories = pd.read_csv(\n",
    "    os.path.join(\"Data\", \"wikispeedia_paths-and-graph\", \"categories.tsv\"),\n",
    "    delimiter=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"article\", \"category\"],\n",
    "    skip_blank_lines=True,\n",
    "    comment=\"#\"\n",
    ")\n",
    "\n",
    "main_categories = []\n",
    "for category in df_categories[\"category\"].values:\n",
    "    main_categories.append(category.split(\".\")[1])\n",
    "\n",
    "df_categories[\"categoryMain\"] = main_categories\n",
    "\n",
    "display(df_categories.head())\n",
    "print(\"Size:\", df_categories.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's merge the continent labels with the article categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continents_categories = pd.merge(df_continents, df_categories, on=\"article\", how=\"left\")\n",
    "\n",
    "display(df_continents_categories.head())\n",
    "print(\"Size:\", df_continents_categories.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a dataset with unique articles, their associated continents, and lists of main and subcategories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles = df_continents_categories[[\"article\", \"continent\"]].drop_duplicates()\n",
    "df_articles = pd.merge(df_articles, df_continents_categories.groupby(\"article\")[\"categoryMain\"].apply(list).reset_index(), on=\"article\")\n",
    "df_articles = pd.merge(df_articles, df_continents_categories.groupby(\"article\")[\"category\"].apply(list).reset_index(), on=\"article\")\n",
    "\n",
    "display(df_articles.head())\n",
    "print(\"Size:\", df_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Article word count\n",
    "\n",
    "Now, we explore the word count of each article in our dataset. We retrieve this information from the plaintext versions of the articles.\n",
    "\n",
    "The resulting dataset includes a new column, `length` representing the word count of each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plaintext_path = os.path.join(\"Data\", \"plaintext_articles\")\n",
    "\n",
    "word_counts = []\n",
    "for article_name in df_articles.article:\n",
    "    file_path = os.path.join(plaintext_path, article_name + \".txt\")\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "\n",
    "        _ = file.readline() # Skip the first line because it contains the word #copyright\n",
    "        content = file.read()\n",
    "\n",
    "    content = content[:re.search(\"Retrieved from\", content).start(0)]\n",
    "    word_counts.append(len(content.split()))\n",
    "\n",
    "df_articles[\"length\"] = word_counts\n",
    "\n",
    "display(df_articles.head())\n",
    "print(\"Size:\", df_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Page Rank\n",
    "\n",
    "In this step, we load the Page Rank data from the `page_rank.csv` file and merge it with our existing dataset. The resulting dataset now includes information about the Page Rank of each article. Page Rank can offer insights into the importance or centrality of an article within the Wikispeedia network.\n",
    "\n",
    "This information will be valuable for our analysis, allowing us to consider the influence and significance of articles when exploring geographical biases in the Wikispeedia game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pagerank = pd.read_csv(os.path.join(\"Data\", \"page_rank.csv\"))\n",
    "df_articles = pd.merge(df_articles, df_pagerank, on=\"article\", how=\"left\").fillna(0)\n",
    "\n",
    "display(df_articles.head())\n",
    "print(\"Size:\", df_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths\n",
    "\n",
    "In this section, we load information about both finished and unfinished paths from the Wikispeedia game. Additional columns are added to facilitate analysis, including the number of backclicks, total path steps, unique articles visited, and whether the path is completed or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_paths_finished = pd.read_csv(\n",
    "    os.path.join(\"Data\", \"wikispeedia_paths-and-graph\", \"paths_finished.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"hashedIpAddress\", \"timestamp\", \"durationInSec\", \"path\", \"rating\"],\n",
    "    skip_blank_lines=True,\n",
    "    comment=\"#\"\n",
    ")\n",
    "df_paths_unfinished = pd.read_csv(\n",
    "    os.path.join(\"Data\", \"wikispeedia_paths-and-graph\", \"paths_unfinished.tsv\"),\n",
    "    sep=\"\\t\",\n",
    "    header=None,\n",
    "    names=[\"hashedIpAddress\", \"timestamp\", \"durationInSec\", \"path\", \"target\", \"motif\"],\n",
    "    skip_blank_lines=True,\n",
    "    comment=\"#\"\n",
    ")\n",
    "\n",
    "df_paths_finished[\"backclicks\"] = df_paths_finished[\"path\"].apply(lambda x: x.count(\"<\"))\n",
    "df_paths_finished[\"pathSteps\"] = df_paths_finished[\"path\"].apply(lambda x: x.count(\";\") + 1)\n",
    "df_paths_finished[\"uniqueArticles\"] = df_paths_finished[\"pathSteps\"] - df_paths_finished[\"backclicks\"]\n",
    "df_paths_finished[\"path\"] = df_paths_finished[\"path\"].apply(lambda x: x.split(\";\"))\n",
    "df_paths_finished[\"start\"] = df_paths_finished[\"path\"].str[0]\n",
    "df_paths_finished[\"target\"] = df_paths_finished[\"path\"].str[-1]\n",
    "df_paths_finished[\"isFinished\"] = True\n",
    "\n",
    "df_paths_unfinished[\"backclicks\"] = df_paths_unfinished[\"path\"].apply(lambda x: x.count(\"<\"))\n",
    "df_paths_unfinished[\"pathSteps\"] = df_paths_unfinished[\"path\"].apply(lambda x: x.count(\";\") + 1)\n",
    "df_paths_unfinished[\"uniqueArticles\"] = df_paths_unfinished[\"pathSteps\"] - df_paths_unfinished[\"backclicks\"]\n",
    "df_paths_unfinished[\"path\"] = df_paths_unfinished[\"path\"].apply(lambda x: x.split(\";\"))\n",
    "df_paths_unfinished[\"start\"] = df_paths_unfinished[\"path\"].str[0]\n",
    "df_paths_unfinished[\"isFinished\"] = False\n",
    "\n",
    "df_paths = pd.concat([df_paths_finished, df_paths_unfinished])\n",
    "df_paths = df_paths[df_paths[\"start\"].isin(df_articles_all.name) & df_paths[\"target\"].isin(df_articles_all.name)]\n",
    "\n",
    "display(df_paths.head())\n",
    "print(\"Size:\", df_paths.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortest Paths\n",
    "\n",
    "Here, we extract information about the shortest paths between articles from the provided file. The resulting dataset, `df_shortest_paths`, is then merged with the existing paths dataset, `df_paths`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_paths = []\n",
    "with open(os.path.join(\"Data\", \"wikispeedia_paths-and-graph\", \"shortest-path-distance-matrix.txt\")) as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line == \"\" or line.startswith(\"#\"):\n",
    "            continue\n",
    "        shortest_paths.append(list(map(lambda x: -1 if x == \"_\" else int(x), list(line))))\n",
    "        \n",
    "shortest_paths = np.array(shortest_paths)\n",
    "\n",
    "df_shortest_paths = pd.DataFrame(shortest_paths, index=df_articles_all.name, columns=df_articles_all.name)\n",
    "df_paths[\"shortestPath\"] = df_paths.apply(lambda row: df_shortest_paths.loc[row[\"start\"], row[\"target\"]], axis=\"columns\")\n",
    "df_paths = df_paths[df_paths[\"shortestPath\"] >= 0]\n",
    "\n",
    "display(df_paths.head())\n",
    "print(\"Size:\", df_paths.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "In this section, we explore the distribution of articles across different continents. The `article_count_per_continent` DataFrame provides a summary of the number of articles in each continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_count_per_continent = df_continents.groupby(\"continent\").size().sort_index()\n",
    "\n",
    "display(article_count_per_continent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we examine the distribution of articles in various main categories within each continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_continents_categories_counts = pd.crosstab(df_continents_categories[\"continent\"], df_continents_categories[\"categoryMain\"]).sort_index()\n",
    "\n",
    "display(df_continents_categories_counts)\n",
    "print(\"Size:\", df_continents_categories_counts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge information about target and start articles with the paths data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_articles_target = df_articles.copy()\n",
    "df_articles_target.columns = [column[0].upper() + column[1:] for column in df_articles_target.columns]\n",
    "df_articles_target = df_articles_target.add_prefix(\"target\")\n",
    "\n",
    "df_paths_articles = pd.merge(df_paths, df_articles_target, left_on=\"target\", right_on=\"targetArticle\", suffixes=[\"\", ]).drop(columns=\"targetArticle\")\n",
    "\n",
    "df_start_articles = df_articles.copy()\n",
    "df_start_articles.columns = [column[0].upper() + column[1:] for column in df_start_articles.columns]\n",
    "df_start_articles = df_start_articles.add_prefix(\"start\")\n",
    "df_paths_articles = pd.merge(df_paths_articles, df_start_articles, left_on=\"start\", right_on=\"startArticle\", suffixes=[\"\", ]).drop(columns=\"startArticle\")\n",
    "\n",
    "df_paths_articles[\"isFinishedInt\"] = df_paths_articles[\"isFinished\"].astype(int)\n",
    "\n",
    "display(df_paths_articles.head())\n",
    "print(\"Size:\", df_paths_articles.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we perform an analysis of article path statistics. The resulting DataFrame, `df_article_path_stats`, contains information about the number of finished and unfinished paths for each article, along with percentages and relevant details. This exploration sets the stage for deeper insights into user interactions with articles in the Wikispeedia game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_article_path_stats = pd.DataFrame()\n",
    "\n",
    "df_article_path_stats[\"article\"] = df_articles[\"article\"]\n",
    "df_article_path_stats[\"continent\"] = df_articles[\"continent\"]\n",
    "df_article_path_stats[\"targetFinished\"] = df_articles[\"article\"].map(df_paths_finished[\"target\"].value_counts()).fillna(0)\n",
    "df_article_path_stats[\"targetUnfinished\"] = df_articles[\"article\"].map(df_paths_unfinished[\"target\"].value_counts()).fillna(0)\n",
    "\n",
    "df_article_path_stats[\"startFinished\"] = df_articles[\"article\"].map(df_paths_finished[\"start\"].value_counts()).fillna(0)\n",
    "df_article_path_stats[\"startUnfinished\"] = df_articles[\"article\"].map(df_paths_unfinished[\"start\"].value_counts()).fillna(0)\n",
    "\n",
    "paths_finished = pd.Series(np.concatenate(df_paths_finished.path.values))\n",
    "paths_unfinished = pd.Series(np.concatenate(df_paths_unfinished.path.values))\n",
    "\n",
    "df_article_path_stats[\"anyFinished\"] = df_articles[\"article\"].map(paths_finished.value_counts()).fillna(0)\n",
    "df_article_path_stats[\"anyUnfinished\"] = df_articles[\"article\"].map(paths_unfinished.value_counts()).fillna(0)\n",
    "df_article_path_stats[\"anyPercentage\"] = (df_article_path_stats[\"anyFinished\"] + df_article_path_stats[\"anyUnfinished\"]) / (len(paths_finished) + len(paths_unfinished))\n",
    "\n",
    "display(df_article_path_stats.sort_values(\"anyPercentage\", ascending=False).head())\n",
    "print(\"Size:\", df_article_path_stats.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Analysis\n",
    "\n",
    "We conduct a naive statistical analysis to identify potential differences between paths leading to articles related to Europe (treatment group) and paths leading to articles related to other continents (control group). The t-tests for metrics such as completion status, duration, path steps, and rating provide initial insights into potential disparities between the two groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_analysis = df_paths_articles.copy()\n",
    "df_analysis = df_analysis.fillna(0)\n",
    "df_analysis[\"treatment\"] = df_analysis.targetContinent == \"Europe\"\n",
    "\n",
    "for col in [\"isFinishedInt\", \"durationInSec\", \"pathSteps\", \"rating\"]:\n",
    "    print(col, *scipy.stats.ttest_ind(df_analysis[df_analysis.treatment][col], df_analysis[~df_analysis.treatment][col], equal_var=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matching\n",
    "\n",
    "Matching is a crucial step in observational studies to control for confounding factors and ensure a fair comparison between the treatment and control groups. In our study, we propose matching based on the following factors:\n",
    "\n",
    "- Starting article\n",
    "- PageRank of the goal (indicating the same probability of reaching the goal)\n",
    "- Same category of the target article\n",
    "\n",
    "Matching allows us to create more comparable groups, reducing bias and increasing the reliability of our analysis. By considering these factors, we aim to create balanced groups that are comparable in terms of key characteristics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation Study\n",
    "\n",
    "This study aims to uncover trends and patterns in the behavior of players when interacting with articles associated with Europe compared to other continents.\n",
    "\n",
    "The matched groups, created through the matching process, provide a controlled environment for analysis.\n",
    "\n",
    "By controlling for confounding factors through matching, we aim to derive meaningful and reliable conclusions about the influence of geographical factors on user experiences in the Wikispeedia game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Story Plots\n",
    "\n",
    "In preparation for the visual exploration of our data story, we generate a set of distinctive colors for each continent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents = df_continents[\"continent\"].unique()\n",
    "random_colors = sns.color_palette(\"husl\", n_colors=len(continents))\n",
    "continents_colors = {}\n",
    "continents_colors_int = {}\n",
    "for i in range(len(continents)):\n",
    "    continents_colors[continents[i]] = random_colors[i]\n",
    "    continents_colors_int[continents[i]] = tuple(map(lambda x: int(255 * x), random_colors[i]))\n",
    "    continents_colors_int[continents[i]] = \"#{0:02x}{1:02x}{2:02x}\".format(*continents_colors_int[continents[i]])\n",
    "print(continents_colors)\n",
    "print(continents_colors_int)\n",
    "\n",
    "CONTINENTS_NUM = len(continents_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 1: Number of Articles per Continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_count_per_continent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_count_per_continent.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization presents the distribution of articles across different continents. The bar chart and pie chart provide a visual representation of the number of articles per continent, offering insights into the dataset's geographical coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_name = \"articles_count_per_continent\"\n",
    "fig_title = \"Number of articles per Continent\"\n",
    "fig_ylabel = \"Count\"\n",
    "fig_xlabel = \"Continent\"\n",
    "\n",
    "\n",
    "fig = px.bar(\n",
    "    x=article_count_per_continent.index,\n",
    "    y=article_count_per_continent.values,\n",
    "    color=[continents_colors_int[continent] for continent in article_count_per_continent.index],\n",
    "    color_discrete_map=\"identity\",\n",
    "    labels={\"index\": fig_ylabel, \"value\": fig_xlabel},\n",
    "\n",
    ")\n",
    "fig.update_layout(\n",
    "    title_text=fig_title,\n",
    "    title_x=0.5,\n",
    "    #xaxis=dict(tickangle=-45),\n",
    "    width=FIGURE_WIDTH,\n",
    "    height=FIGURE_HEIGHT,\n",
    ")\n",
    "fig.write_image(os.path.join(PLOTS_PATH_PX, f\"{fig_name}_bar.pdf\"))\n",
    "fig.write_html(os.path.join(PLOTS_PATH_HTML, f\"{fig_name}_bar.html\"))\n",
    "fig.show()\n",
    "\n",
    "pull = np.zeros_like(article_count_per_continent.index) + 0.1 * (article_count_per_continent.index == EUROPE_LABEL)\n",
    "fig = go.Figure(data=[go.Pie(\n",
    "    values=article_count_per_continent.values,\n",
    "    labels=article_count_per_continent.index.tolist(),\n",
    "    pull=pull.tolist(),\n",
    "    marker_colors=[continents_colors_int[continent] for continent in article_count_per_continent.index],\n",
    "    sort=False\n",
    ")])\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=fig_title,\n",
    "    title_x=0.5,\n",
    "    width=FIGURE_WIDTH,\n",
    "    height=FIGURE_HEIGHT,\n",
    ")\n",
    "fig.write_image(os.path.join(PLOTS_PATH_PX, f\"{fig_name}_pie.pdf\"))\n",
    "fig.write_html(os.path.join(PLOTS_PATH_HTML, f\"{fig_name}_pie.html\"))\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 2: Continent Distribution per Category\n",
    "\n",
    "This visualization explores the distribution of articles across different continents within various categories. The bar chart provides an overview of the article count per category, while the interactive pie chart allows users to select specific categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_name = \"articles_count_per_category\"\n",
    "fig_title = \"Continent distribution per Category\"\n",
    "fig_xlabel = \"Article Count\"\n",
    "fig_ylabel = \"Category\"\n",
    "\n",
    "\n",
    "categories_sorted = df_continents_categories_counts.sum(axis=\"index\").sort_values().index\n",
    "\n",
    "fig = px.bar(\n",
    "    df_continents_categories_counts.T.loc[categories_sorted],\n",
    "    orientation =\"h\",\n",
    "    title=fig_title,\n",
    "    labels={\"index\": fig_ylabel, \"value\": fig_xlabel},\n",
    "    color_discrete_sequence=[continents_colors_int[continent] for continent in df_continents_categories_counts.index],\n",
    ")\n",
    "fig.update_layout(\n",
    "    legend_title_text=\"\",\n",
    "    title_x=0.5,\n",
    "    width=FIGURE_WIDTH,\n",
    "    height=FIGURE_HEIGHT    \n",
    ")\n",
    "fig.write_html(os.path.join(PLOTS_PATH_HTML, f\"{fig_name}_bar.html\"))\n",
    "fig.write_image(os.path.join(PLOTS_PATH_PX, f\"{fig_name}_bar.pdf\"))\n",
    "fig.show()\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "annotations = {}\n",
    "buttons = []\n",
    "visible = True\n",
    "mask = [False] * len(categories_sorted)\n",
    "max_name_len = max(len(name) for name in continents)\n",
    "for category_idx, category in enumerate(reversed(categories_sorted)):\n",
    "    category_data = df_continents_categories_counts[category]\n",
    "    category_data = category_data[category_data > 0]\n",
    "\n",
    "    category_name = category.replace(\"_\", \" \")\n",
    "    labels = [f\"{name : <{max_name_len}}\" for name in category_data.index]\n",
    "\n",
    "    pull = np.zeros_like(category_data.index) + 0.1 * (category_data.index == EUROPE_LABEL)\n",
    "    fig.add_trace(go.Pie(\n",
    "        labels=labels,\n",
    "        values=category_data.values,\n",
    "        pull=pull.tolist(),\n",
    "        marker_colors=[continents_colors_int[continent] for continent in category_data.index],\n",
    "        visible=visible,\n",
    "        name=category_name,\n",
    "        sort=False\n",
    "    ))\n",
    "\n",
    "    annotation = dict(\n",
    "        text=f\"Category: {category_name}\",\n",
    "        x=-0.3,\n",
    "        y=0.05,\n",
    "        xanchor=\"left\",\n",
    "        showarrow=False\n",
    "    )\n",
    "    if visible:\n",
    "        fig.add_annotation(annotation)\n",
    "\n",
    "    mask[category_idx] = True\n",
    "    buttons.append(dict(\n",
    "        label=category_name,\n",
    "        method=\"update\",\n",
    "        args=[\n",
    "            {\"visible\": list(mask)},\n",
    "            {\"title\": fig_title, \"annotations\": [annotation]}\n",
    "        ]\n",
    "    ))\n",
    "    mask[category_idx] = False\n",
    "    visible=False\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=fig_title,\n",
    "    title_x=0.7,\n",
    "    width=FIGURE_WIDTH,\n",
    "    height=FIGURE_HEIGHT,\n",
    "    legend=dict(\n",
    "        x=-0.3,\n",
    "        y=0.1\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "fig.update_layout(\n",
    "    updatemenus=[\n",
    "        dict(\n",
    "            active=0,\n",
    "            buttons=buttons\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.write_html(os.path.join(PLOTS_PATH_HTML, f\"{fig_name}_pie.html\"))\n",
    "fig.write_image(os.path.join(PLOTS_PATH_PX, f\"{fig_name}_pie.pdf\"))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
